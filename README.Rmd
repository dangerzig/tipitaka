---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# tipitaka

<!-- badges: start -->
<!-- badges: end -->

The goal of tipitaka is to allow students and researchers to apply the tools of computational linguistics to the ancient Buddhist texts known as the Tipitaka or Pali Canon.

The Tipitaka is the canonical scripture of Theravadin Buddhists worldwide. It purports to record the direct teachings of the historical Buddha. It was first recorded in written form in what is now Sri Lanka, likely around 100 BCE.

The tipitaka package provides the texts of the Tipitaka in various electronic forms, plus functions for working with the Pali language.

## What's New in Version 1.0

Version 1.0 adds a **critical edition** of the Sutta Pitaka based on a five-witness collation:

* **PTS** (Pali Text Society editions via GRETIL) — the base text
* **SuttaCentral** (Mahāsaṅgīti edition)
* **VRI** (Chaṭṭha Saṅgāyana / CST4)
* **BJT** (Buddha Jayanti Tipitaka, Sri Lanka)
* **Thai** (Syām Raṭṭha Royal Thai Edition)

Key improvements:

* **Lemmatization**: Words are grouped by dictionary headword using the [Digital Pali Dictionary](https://digitalpalidictionary.github.io/) (DPD), achieving 99.78% token-level coverage. For example, "buddhassa", "buddho", and "buddhaṃ" are all counted under "buddha".

* **Sutta-level granularity**: Word frequencies are available for each individual sutta (~5,700 suttas), not just by volume.

* **Corrections**: Where multiple witnesses (SC, VRI, BJT) agree against PTS and the PTS reading is not a valid Pali word per DPD, the text is corrected and the original PTS reading recorded.

## Data Sources

The package includes two data sources:

**VRI Edition** (original datasets): The complete Tipitaka from the Chattha Sangāyana Tipiṭaka version 4.0 (CST4) published by the Vipassana Research Institute. This covers all three pitakas (Vinaya, Sutta, and Abhidhamma).

* `tipitaka_raw`, `tipitaka_long`, `tipitaka_wide` - Full canon, volume-level

**Critical Edition** (new in v1.0): A lemmatized critical edition of the Sutta Pitaka only.

* `tipitaka_long_critical`, `tipitaka_wide_critical` - Lemma frequencies by nikaya
* `tipitaka_suttas_long`, `tipitaka_suttas_wide` - Lemma frequencies by sutta
* `tipitaka_suttas_raw` - Full text per sutta
* `tipitaka_long_words` - Surface forms (non-lemmatized) for comparison

## Pali Alphabet

There is no universal script for Pali; traditionally each Buddhist country uses its own script to write Pali phonetically. This package uses the Roman script and the diacritical system developed by the PTS. However, note that the Pali alphabet does NOT follow the alphabetical ordering of English or other Roman-script languages. For this reason, tipitaka includes `pali_alphabet` giving the full Pali alphabet in order, and the functions `pali_lt`, `pali_gt`, `pali_eq`, and `pali_sort` for comparing and sorting Pali strings.

## Installation

You can install the released version of tipitaka from [CRAN](https://CRAN.R-project.org) with:

``` r
install.packages("tipitaka")
```

And the development version from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("dangerzig/tipitaka")
```
## Example

You can use tipitaka to do clustering analysis of the various books of the Pali Canon. For example:

```{r dendogram, message = FALSE, warning = FALSE}
library(tipitaka)
dist_m <- dist(tipitaka_wide)
cluster <- hclust(dist_m)
plot(cluster)
```

You can also create traditional k-means clusters and visualize these using packages like `factoextra`:

```{r kmeans, message = FALSE, warning = FALSE}
library(factoextra) # great visualizer for clusters
km <- kmeans(dist_m, 2, nstart = 25, algorithm = "Lloyd")
fviz_cluster(km, dist_m, labelsize = 12, repel = TRUE)
```

You can extract data for any individual sutta from the critical edition and explore it with packages like `wordcloud`. Here we look at the Mahasatipatthana Sutta (DN 22):

```{r wordclouds, message = FALSE, warning = FALSE}
library(wordcloud)
# Extract lemma frequencies for DN 22
dn22 <- tipitaka_suttas_long[tipitaka_suttas_long$sutta == "dn22", ]
# Remove stop words and plot
dn22_content <- dn22[!dn22$word %in% pali_stop_words$word, ]
with(dn22_content, wordcloud(word, n, max.words = 40))
```

Finally, we can look at word frequency by rank:
```{r freq-by-word, message = FALSE, warning = FALSE}
library(dplyr, quietly = TRUE)
freq_by_rank <- tipitaka_long %>%
  group_by(word) %>%
  add_count(wt = n, name = "word_total") %>%
  ungroup() %>%
  distinct(word, .keep_all = TRUE) %>%
  mutate(tipitaka_total =
           sum(distinct(tipitaka_long, book,
                        .keep_all = TRUE)$total)) %>%
    transform(freq = word_total/tipitaka_total) %>%
  arrange(desc(freq)) %>%
  mutate(rank = row_number()) %>%
  select(-n, -total, -book)

freq_by_rank %>%
  ggplot(aes(rank, freq)) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()
```

## Lemmatized Analysis (New in v1.0)

The lemmatized critical edition enables more meaningful clustering by grouping inflected forms:

```{r dendogram-critical, message = FALSE, warning = FALSE}
# Cluster the five nikayas using lemmatized data
dist_critical <- dist(tipitaka_wide_critical)
hc_critical <- hclust(dist_critical)
plot(hc_critical, main = "Nikaya Clustering (Lemmatized)")
```

You can search for specific lemmas across all suttas:

```{r search-lemma, message = FALSE, warning = FALSE}
# Find suttas that mention "nibbana" most frequently
nibbana <- search_lemma("nibbana")
head(nibbana[, c("sutta", "nikaya", "n", "freq")], 10)
```

### Comparing Lemmatized vs. Surface Forms

The critical edition includes both lemmatized (`tipitaka_long_critical`) and non-lemmatized (`tipitaka_long_words`) word counts, making it easy to see the effect of lemmatization:

```{r lemma-comparison, message = FALSE, warning = FALSE}
# Surface forms in the Digha Nikaya
dn_words <- tipitaka_long_words[tipitaka_long_words$book == "dn", ]
# Lemmatized forms
dn_lemmas <- tipitaka_long_critical[tipitaka_long_critical$book == "dn", ]
cat("DN surface forms:", nrow(dn_words), "\n")
cat("DN unique lemmas:", nrow(dn_lemmas), "\n")
cat("Reduction:", round(100 * (1 - nrow(dn_lemmas) / nrow(dn_words)), 1), "%\n")
```

### Extracting Sutta Text

The `tipitaka_suttas_raw` dataset provides the full Pali text of every sutta. For example, the opening of the Brahmajala Sutta (DN 1):

```{r sutta-text, message = FALSE, warning = FALSE}
dn1_text <- tipitaka_suttas_raw[tipitaka_suttas_raw$sutta == "dn1", "text"]
# Show the first 200 characters
cat(substr(dn1_text, 1, 200), "...\n")
```

### Sutta-Level Clustering

With sutta-level data, you can cluster individual suttas within a nikaya:

```{r sutta-clustering, message = FALSE, warning = FALSE}
# Cluster all 34 Digha Nikaya suttas
dn_suttas <- tipitaka_suttas_wide[grep("^dn", rownames(tipitaka_suttas_wide)), ]
dist_dn <- dist(dn_suttas)
hc_dn <- hclust(dist_dn)
plot(hc_dn, main = "Digha Nikaya Sutta Clustering", cex = 0.7)
```

### Cross-Nikaya Vocabulary Comparison

Which lemmas are most distinctive to each nikaya? Here we find lemmas that appear much more frequently in one nikaya than the overall average:

```{r distinctive-lemmas, message = FALSE, warning = FALSE}
library(dplyr, quietly = TRUE)
# Calculate overall frequency for each lemma
overall <- tipitaka_long_critical %>%
  group_by(word) %>%
  summarise(overall_freq = sum(n) / sum(total), .groups = "drop")

# Find the most distinctive lemma in each nikaya
distinctive <- tipitaka_long_critical %>%
  inner_join(overall, by = "word") %>%
  mutate(ratio = freq / overall_freq) %>%
  filter(n >= 100) %>%  # only common words
  group_by(book) %>%
  slice_max(ratio, n = 5) %>%
  select(nikaya = book, lemma = word, freq, overall_freq, ratio)

distinctive
```

